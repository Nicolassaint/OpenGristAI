# Function Calling Diagnostics & Troubleshooting

Ce document explique le syst√®me de diagnostics am√©lior√© pour le function calling dans Grist AI API v2.

## üéØ Objectif

Avec la diversit√© des mod√®les LLM disponibles (OpenAI, Mistral, Claude, Groq, etc.), tous ne supportent pas le function calling de la m√™me mani√®re. Ce syst√®me de diagnostics permet de :

1. **D√©tecter** rapidement si un mod√®le ne supporte pas le function calling
2. **Diagnostiquer** les probl√®mes de compatibilit√© avec des logs d√©taill√©s
3. **Suivre** la sant√© du syst√®me avec des m√©triques
4. **Alerter** en cas de comportements suspects

## üîç Syst√®me de Logging Am√©lior√©

### Niveaux de Logs

Le syst√®me utilise plusieurs niveaux de logs :

- **DEBUG** : D√©tails techniques de chaque it√©ration
- **INFO** : √âv√©nements importants (tool calls r√©ussis, m√©triques)
- **WARNING** : Comportements suspects (pas de tool calls, liste vide)
- **ERROR** : Probl√®mes critiques (function calling non support√©)

### Ce qui est logg√© maintenant

#### Pour chaque it√©ration de l'agent

```
DEBUG: LLM Response Type: AIMessage
DEBUG: Response has 'content': True
DEBUG: Response has 'tool_calls': True
DEBUG: tool_calls attribute type: <class 'list'>
DEBUG: tool_calls value: [...]
DEBUG: tool_calls bool value: True
DEBUG: Number of tool calls: 2
INFO:  ‚úì LLM requested 2 tool call(s) - function calling working correctly
```

#### Pour chaque tool call

```
DEBUG: --- Tool Call 1/2 ---
DEBUG: Tool: get_tables
DEBUG: Args: {}
DEBUG: ID: call_abc123
INFO:  ‚úÖ Tool 'get_tables' executed successfully
```

#### En cas d'√©chec

```
ERROR: üî¥ MALFORMED TOOL CALL: Unable to parse tool call structure
ERROR: ‚ùå Tool 'get_tables' failed: Connection timeout
```

#### D√©tection de probl√®mes

```
WARNING: ‚ö†Ô∏è  LLM returned empty tool_calls list. This may indicate the model doesn't understand function calling properly.

ERROR: üî¥ FUNCTION CALLING FAILURE: No tool calls for 3 consecutive iterations. 
This strongly suggests the LLM (mistral-small) doesn't properly support function calling.
Consider using a different model (e.g., gpt-4, gpt-3.5-turbo, claude-3-sonnet, mistral-large-latest).
```

### M√©triques de fin d'ex√©cution

√Ä la fin de chaque requ√™te, un r√©sum√© est fourni :

```
INFO: ‚úÖ Agent execution completed successfully
INFO: üìä Execution Summary:
INFO:    - Total iterations: 3/10
INFO:    - Total tool calls: 5
INFO:    - Failed tool calls: 0
INFO:    - Success rate: 100.0%
```

## üß™ Validation au D√©marrage

### Validation manuelle

Vous pouvez valider qu'un mod√®le supporte le function calling avec :

```python
from app.core.agent import GristAgent

agent = GristAgent(
    document_id="your_doc_id",
    grist_token="your_token"
)

# Valider le function calling
validation_result = await agent.validate_function_calling()

if validation_result["test_passed"]:
    print("‚úÖ Function calling is working correctly")
else:
    print(f"‚ùå Problem detected: {validation_result}")
```

### Validation automatique

Pour activer la validation automatique au premier `run()` :

```python
agent = GristAgent(
    document_id="your_doc_id",
    grist_token="your_token",
    validate_function_calling_on_init=True  # Active la validation
)

# La validation se fera automatiquement au premier run
result = await agent.run("Liste les tables")
```

‚ö†Ô∏è **Note** : La validation fait un appel API test au LLM, ce qui ajoute :
- Latence (~1-2 secondes)
- Co√ªt minimal (1 appel API simple)

### R√©sultat de validation

La validation retourne un dictionnaire avec :

```python
{
    "supported": True,           # Si le mod√®le a l'attribut tool_calls
    "has_tool_calls_attr": True, # Si response.tool_calls existe
    "test_passed": True,         # Si le test complet a r√©ussi
    "response_type": "AIMessage",# Type de la r√©ponse
    "num_tool_calls": 1,        # Nombre de tool calls g√©n√©r√©s
}
```

## üìä M√©triques et Diagnostics

### M√©triques retourn√©es

Chaque appel √† `agent.run()` retourne maintenant des m√©triques :

```python
result = await agent.run("Ajoute une colonne Age")

print(result["metrics"])
# {
#     "iterations": 2,
#     "tool_calls": 3,
#     "failed_tool_calls": 0
# }
```

### Patterns de probl√®mes d√©tect√©s

#### 1. Mod√®le ne supporte pas function calling

**Sympt√¥mes** :
- Pas d'attribut `tool_calls` dans la r√©ponse
- 0 tool calls sur toutes les it√©rations

**Logs** :
```
WARNING: ‚ö†Ô∏è  LLM response has no 'tool_calls' attribute. This model may not support function calling.
ERROR: üî¥ CRITICAL: Agent made 0 tool calls across all iterations.
```

**Solution** : Changer de mod√®le vers un mod√®le compatible

#### 2. Mod√®le supporte partiellement

**Sympt√¥mes** :
- Attribut `tool_calls` pr√©sent mais souvent vide
- Quelques tool calls mais pas toujours

**Logs** :
```
WARNING: ‚ö†Ô∏è  LLM returned empty tool_calls list.
```

**Solution** : Utiliser un mod√®le plus performant ou ajuster le prompt syst√®me

#### 3. Tools √©chouent fr√©quemment

**Sympt√¥mes** :
- Taux d'√©chec > 50%
- Erreurs r√©p√©t√©es d'ex√©cution

**Logs** :
```
ERROR: ‚ùå Tool 'query_document' failed: Invalid SQL syntax
ERROR: üî¥ CRITICAL: High failure rate (3/5).
```

**Solution** : V√©rifier les arguments pass√©s par le LLM, am√©liorer les descriptions des tools

#### 4. Max iterations atteint

**Sympt√¥mes** :
- Agent atteint la limite d'it√©rations
- Pas de r√©ponse finale

**Logs** :
```
ERROR: ‚ö†Ô∏è  Agent reached max iterations (10)
```

**Solution** : 
- Augmenter `max_iterations` si besoin
- Simplifier la requ√™te
- V√©rifier si le mod√®le boucle (m√™me tools appel√©s r√©p√©titivement)

## üîß Configuration

### Variables d'environnement

```bash
# Mod√®le √† utiliser
OPENAI_MODEL=gpt-4-turbo-preview

# URL de base (pour providers compatibles OpenAI)
OPENAI_BASE_URL=https://api.openai.com/v1

# Cl√© API
OPENAI_API_KEY=your_api_key

# Agent settings
AGENT_MAX_ITERATIONS=10
AGENT_VERBOSE=true
```

### Niveau de logging

Dans votre code Python :

```python
import logging

# Activer tous les logs de debug
logging.basicConfig(level=logging.DEBUG)

# Ou seulement pour le module agent
logging.getLogger("app.core.agent").setLevel(logging.DEBUG)
```

## üß™ Compatibilit√© des Mod√®les

### Mod√®les Test√©s

| Provider | Mod√®le | Function Calling | Qualit√© | Notes |
|----------|--------|------------------|---------|-------|
| OpenAI | gpt-4, gpt-4-turbo | ‚úÖ Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | R√©f√©rence |
| OpenAI | gpt-3.5-turbo | ‚úÖ Bon | ‚≠ê‚≠ê‚≠ê‚≠ê | Bon rapport qualit√©/prix |
| Anthropic | claude-3-opus | ‚úÖ Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Via tool use API |
| Anthropic | claude-3-sonnet | ‚úÖ Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Recommand√© |
| Mistral | mistral-large-latest | ‚úÖ Bon | ‚≠ê‚≠ê‚≠ê‚≠ê | Compatible |
| Mistral | mistral-medium-latest | ‚ö†Ô∏è Moyen | ‚≠ê‚≠ê‚≠ê | Peut oublier des tools |
| Mistral | mistral-small, 7B, etc. | ‚ùå Non | - | Pas de support |
| Groq | llama-3.1-70b-versatile | ‚úÖ Bon | ‚≠ê‚≠ê‚≠ê‚≠ê | Rapide |
| Groq | mixtral-8x7b-32768 | ‚ö†Ô∏è Moyen | ‚≠ê‚≠ê‚≠ê | Support partiel |

### Comment tester un nouveau mod√®le

1. **Configurer le mod√®le** :
```bash
export OPENAI_MODEL=mistral-large-latest
export OPENAI_BASE_URL=https://api.mistral.ai/v1
export OPENAI_API_KEY=your_mistral_key
```

2. **Tester avec validation** :
```python
agent = GristAgent(
    document_id="test_doc",
    grist_token="test_token",
    validate_function_calling_on_init=True
)

result = await agent.validate_function_calling()
print(f"Validation: {result}")
```

3. **Faire quelques tests** :
```python
# Test simple
result1 = await agent.run("Liste les tables")

# Test avec tool calls multiples
result2 = await agent.run("Ajoute une colonne Age de type Int √† la table Users")

# V√©rifier les m√©triques
print(f"M√©triques: {result1['metrics']}")
print(f"M√©triques: {result2['metrics']}")
```

4. **Analyser les logs** :
- V√©rifier qu'il y a des tool calls
- V√©rifier le taux de succ√®s
- Regarder s'il y a des warnings

## üêõ Troubleshooting

### Probl√®me : Aucun tool call g√©n√©r√©

**Diagnostic** :
```bash
# Activer les logs d√©taill√©s
export AGENT_VERBOSE=true

# V√©rifier les logs
tail -f logs/app.log | grep "tool_calls"
```

**V√©rifications** :
1. Le mod√®le supporte-t-il function calling ?
2. La cl√© API est-elle valide ?
3. Le `base_url` est-il correct ?

### Probl√®me : Tools appel√©s mais √©chouent

**Diagnostic** :
```bash
# Regarder les erreurs sp√©cifiques
tail -f logs/app.log | grep "ERROR"
```

**V√©rifications** :
1. Les descriptions des tools sont-elles claires ?
2. Le Grist token est-il valide ?
3. Les donn√©es respectent-elles le sch√©ma ?

### Probl√®me : Max iterations atteint syst√©matiquement

**Diagnostic** :
```python
# Augmenter les iterations temporairement pour debug
agent = GristAgent(
    ...,
    max_iterations=20,  # Au lieu de 10
    verbose=True
)
```

**V√©rifications** :
1. Le mod√®le boucle-t-il (m√™mes tools r√©p√©t√©s) ?
2. Les tool results sont-ils utiles pour le LLM ?
3. La requ√™te est-elle trop complexe ?

## üìù Exemples d'Utilisation

### Exemple 1 : Debug d'un nouveau mod√®le

```python
import logging
from app.core.agent import GristAgent

# Activer les logs d√©taill√©s
logging.basicConfig(level=logging.DEBUG)

# Cr√©er l'agent avec validation
agent = GristAgent(
    document_id="test_doc",
    grist_token="token",
    validate_function_calling_on_init=True,
    max_iterations=5
)

# Tester
result = await agent.run("Liste les tables du document")

# Analyser
if not result["success"]:
    print(f"√âchec : {result['error']}")
    print(f"M√©triques : {result['metrics']}")
else:
    print(f"Succ√®s ! M√©triques : {result['metrics']}")
```

### Exemple 2 : Monitoring en production

```python
from app.core.agent import GristAgent
import logging

logger = logging.getLogger(__name__)

async def handle_user_request(request):
    agent = GristAgent(...)
    
    result = await agent.run(request.message)
    
    # Logger les m√©triques
    metrics = result.get("metrics", {})
    logger.info(f"Request completed", extra={
        "tool_calls": metrics.get("tool_calls", 0),
        "failed_calls": metrics.get("failed_tool_calls", 0),
        "iterations": metrics.get("iterations", 0),
        "success": result["success"]
    })
    
    # Alerter si probl√®me
    if not result["success"] or metrics.get("failed_tool_calls", 0) > 0:
        logger.warning(f"Request had issues: {result}")
    
    return result
```

## üìö R√©f√©rences

- [LangChain Function Calling](https://python.langchain.com/docs/modules/model_io/chat/function_calling)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [Mistral Function Calling](https://docs.mistral.ai/capabilities/function_calling/)
- [Anthropic Tool Use](https://docs.anthropic.com/claude/docs/tool-use)

---

**Cr√©√© le** : 2025-01-19  
**Derni√®re mise √† jour** : 2025-01-19  
**Version API** : v2

