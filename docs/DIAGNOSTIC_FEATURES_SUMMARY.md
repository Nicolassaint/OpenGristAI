# R√©sum√© des Fonctionnalit√©s de Diagnostic

## üéØ Probl√®me R√©solu

**Avant** : Lorsqu'un mod√®le LLM ne supportait pas correctement le function calling, l'application √©chouait silencieusement sans indication claire du probl√®me.

**Maintenant** : Le syst√®me d√©tecte automatiquement les probl√®mes de compatibilit√© et fournit des diagnostics d√©taill√©s avec des recommandations.

## üì¶ Nouveaut√©s

### 1. Logging D√©taill√© dans `agent.py`

#### Ce qui est logg√© maintenant :

```python
# Pour chaque it√©ration
- Type de r√©ponse LLM (AIMessage, etc.)
- Pr√©sence de l'attribut tool_calls
- Valeur et type de tool_calls
- Nombre de tool calls demand√©s

# Pour chaque tool call
- Nom du tool
- Arguments
- ID du call
- R√©sultat (tronqu√© si long)
- Succ√®s/√©chec avec √©mojis visuels

# D√©tection automatique de probl√®mes
- Pas de tool calls pendant 3+ it√©rations
- Liste tool_calls vide
- Taux d'√©chec √©lev√© (>50%)
- Max iterations atteint
```

#### M√©triques Retourn√©es

Chaque appel √† `agent.run()` retourne maintenant :

```python
{
    "output": "...",
    "success": True,
    "intermediate_steps": [...],
    "metrics": {
        "iterations": 2,
        "tool_calls": 5,
        "failed_tool_calls": 0,
        "iterations_without_tools": 0  # Si max iterations atteint
    }
}
```

### 2. Validation de Function Calling dans `llm.py`

#### Nouvelle fonction : `validate_function_calling()`

Test automatique avec un tool simple pour v√©rifier :
- Le mod√®le supporte-t-il `bind_tools()` ?
- La r√©ponse contient-elle un attribut `tool_calls` ?
- Le mod√®le g√©n√®re-t-il r√©ellement des tool calls ?

```python
result = await validate_function_calling(llm, "gpt-4")
# Returns:
# {
#     "supported": True,
#     "has_tool_calls_attr": True,
#     "test_passed": True,
#     "response_type": "AIMessage",
#     "num_tool_calls": 1
# }
```

### 3. M√©thode de Validation dans `GristAgent`

#### Usage manuel

```python
agent = GristAgent(...)
result = await agent.validate_function_calling()

if result["test_passed"]:
    print("‚úÖ Mod√®le compatible")
else:
    print("‚ùå Probl√®me d√©tect√©")
```

#### Usage automatique

```python
agent = GristAgent(
    ...,
    validate_function_calling_on_init=True  # Valide au premier run()
)
```

### 4. D√©tection de Patterns de Probl√®mes

Le syst√®me d√©tecte automatiquement :

| Pattern | Sympt√¥me | Action |
|---------|----------|--------|
| **Pas de support** | 0 tool calls, pas d'attribut tool_calls | ERROR avec recommandations de mod√®les |
| **Support partiel** | tool_calls parfois vide | WARNING de vigilance |
| **Haute d√©faillance** | >50% d'√©checs | ERROR avec diagnostic |
| **Boucle infinie** | Max iterations atteint | WARNING avec m√©triques |

## üìÅ Fichiers Modifi√©s

### Core
- ‚úÖ `app/core/agent.py` - Logging d√©taill√© + m√©triques + d√©tection de probl√®mes
- ‚úÖ `app/core/llm.py` - Fonction de validation

### Documentation
- ‚úÖ `docs/FUNCTION_CALLING_DIAGNOSTICS.md` - Guide complet
- ‚úÖ `docs/README.md` - Index de la documentation
- ‚úÖ `docs/DIAGNOSTIC_FEATURES_SUMMARY.md` - Ce fichier
- ‚úÖ `CHANGELOG.md` - Mis √† jour

### Tests & Exemples
- ‚úÖ `tests/test_function_calling_validation.py` - Script de test
- ‚úÖ `examples/test_diagnostics.py` - Exemples d'utilisation

## üöÄ Comment Utiliser

### Sc√©nario 1 : Tester un nouveau mod√®le

```bash
# 1. Configurer le mod√®le
export OPENAI_MODEL=mistral-large-latest
export OPENAI_BASE_URL=https://api.mistral.ai/v1
export OPENAI_API_KEY=your_key

# 2. Tester
python tests/test_function_calling_validation.py

# 3. Interpr√©ter les r√©sultats
# ‚úÖ PASSED = Compatible
# ‚ö†Ô∏è  UNCERTAIN = √Ä tester en profondeur
# ‚ùå FAILED = Non compatible
```

### Sc√©nario 2 : Debugging d'un probl√®me

```bash
# 1. Activer les logs d√©taill√©s
export AGENT_VERBOSE=true

# 2. Logger niveau DEBUG
export LOG_LEVEL=DEBUG

# 3. Lancer l'application
python -m app.api.main

# 4. Faire une requ√™te et examiner les logs
# Regarder pour :
# - "tool_calls attribute type"
# - "Number of tool calls"
# - "‚ö†Ô∏è" et "üî¥" warnings/errors
```

### Sc√©nario 3 : Monitoring en production

```python
# Dans votre code
result = await agent.run(user_message)

# Logger les m√©triques
logger.info(
    "Request completed",
    extra={
        "tool_calls": result["metrics"]["tool_calls"],
        "failed_calls": result["metrics"]["failed_tool_calls"],
        "iterations": result["metrics"]["iterations"],
    }
)

# Alerter si probl√®me
if result["metrics"]["failed_tool_calls"] > 0:
    alert_team("Agent having issues", result["metrics"])
```

## üîç Exemples de Logs

### Cas Normal (Tout fonctionne)

```
INFO: ‚úì LLM requested 2 tool call(s) - function calling working correctly
DEBUG: --- Tool Call 1/2 ---
DEBUG: Tool: get_tables
INFO: ‚úÖ Tool 'get_tables' executed successfully
INFO: ‚úÖ Agent execution completed successfully
INFO: üìä Execution Summary:
INFO:    - Total iterations: 2/10
INFO:    - Total tool calls: 2
INFO:    - Failed tool calls: 0
INFO:    - Success rate: 100.0%
```

### Cas Probl√©matique (Mod√®le incompatible)

```
DEBUG: LLM Response Type: AIMessage
DEBUG: Response has 'content': True
DEBUG: Response has 'tool_calls': False
WARNING: ‚ö†Ô∏è  LLM response has no 'tool_calls' attribute.
WARNING: This model may not support function calling.
ERROR: üî¥ FUNCTION CALLING FAILURE: No tool calls for 3 consecutive iterations.
ERROR: This strongly suggests the LLM (llama-2-7b) doesn't properly support function calling.
ERROR: Consider using a different model (e.g., gpt-4, gpt-3.5-turbo, claude-3-sonnet, mistral-large-latest).
```

## üéì Ce que Vous Apprenez

Avec ces logs, vous pouvez maintenant diagnostiquer :

1. **Le mod√®le ne supporte pas function calling**
   - Sympt√¥me : Pas d'attribut `tool_calls`
   - Log : "LLM response has no 'tool_calls' attribute"

2. **Le mod√®le supporte mais ne l'utilise pas**
   - Sympt√¥me : Attribut pr√©sent mais liste vide
   - Log : "LLM returned empty tool_calls list"

3. **Les tools √©chouent**
   - Sympt√¥me : Taux d'√©chec √©lev√©
   - Log : "‚ùå Tool 'xxx' failed" + stack trace

4. **Le mod√®le boucle**
   - Sympt√¥me : Max iterations atteint
   - Log : Voir les tool calls r√©p√©t√©s

## üìä Compatibilit√© Test√©e

| Provider | Mod√®le | Status | Notes |
|----------|--------|--------|-------|
| OpenAI | gpt-4 | ‚úÖ | R√©f√©rence |
| OpenAI | gpt-3.5-turbo | ‚úÖ | Bon |
| Mistral | mistral-large | ‚úÖ | Compatible |
| Mistral | mistral-small | ‚ùå | Non support√© |
| Anthropic | claude-3-sonnet | ‚úÖ | Excellent |
| Groq | llama-3.1-70b | ‚úÖ | Rapide |

## üõ†Ô∏è Configuration

### Variables d'Environnement

```bash
# Mod√®le et provider
OPENAI_MODEL=gpt-4
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=sk-...

# Agent
AGENT_MAX_ITERATIONS=10
AGENT_VERBOSE=true

# Logging
LOG_LEVEL=INFO  # ou DEBUG pour plus de d√©tails
```

### Dans le Code

```python
# Option 1: Validation automatique (recommand√© pour nouveaux mod√®les)
agent = GristAgent(
    document_id="...",
    grist_token="...",
    validate_function_calling_on_init=True,  # ‚Üê Active la validation
    max_iterations=10,
    verbose=True
)

# Option 2: Validation manuelle
agent = GristAgent(...)
validation_result = await agent.validate_function_calling()
if not validation_result["test_passed"]:
    raise ValueError("Model not compatible")
```

## üìö Documentation Compl√®te

Pour plus de d√©tails :

- **[FUNCTION_CALLING_DIAGNOSTICS.md](./FUNCTION_CALLING_DIAGNOSTICS.md)** - Guide complet avec troubleshooting
- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - Architecture g√©n√©rale
- **[CHANGELOG.md](../CHANGELOG.md)** - Historique des changements

## ü§ù Contribution

Ces fonctionnalit√©s sont con√ßues pour √™tre extensibles. Contributions bienvenues pour :

- Tests de nouveaux mod√®les
- Am√©lioration des d√©tections de patterns
- Nouveaux diagnostics
- Am√©lioration de la documentation

Voir [CONTRIBUTING.md](../CONTRIBUTING.md).

---

**Cr√©√© le** : 2025-01-19  
**Auteur** : OpenGristAI Team  
**Version** : 0.3.0 (unreleased)

