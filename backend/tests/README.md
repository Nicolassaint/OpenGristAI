# OpenGristAI - Guide des Tests

Ce guide explique comment exÃ©cuter les tests de l'application OpenGristAI.

## Table des MatiÃ¨res

- [Structure des Tests](#structure-des-tests)
- [Installation](#installation)
- [ExÃ©cution des Tests](#exÃ©cution-des-tests)
- [Types de Tests](#types-de-tests)
- [Tests avec API RÃ©elle](#tests-avec-api-rÃ©elle)
- [Configuration](#configuration)

## Structure des Tests

```
tests/
â”œâ”€â”€ conftest.py              # Fixtures et configuration partagÃ©es
â”œâ”€â”€ pytest.ini               # Configuration pytest
â”œâ”€â”€ unit/                    # Tests unitaires (mocked)
â”‚   â”œâ”€â”€ test_agent.py
â”‚   â”œâ”€â”€ test_grist_service.py
â”‚   â”œâ”€â”€ test_tools.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_preview_service.py
â”‚   â”œâ”€â”€ test_validation.py
â”‚   â””â”€â”€ test_confirmation_service.py
â””â”€â”€ integration/             # Tests d'intÃ©gration
    â”œâ”€â”€ test_agent_workflows.py
    â”œâ”€â”€ test_api_endpoints.py
    â””â”€â”€ test_all_tools.py
```

## Installation

Installez les dÃ©pendances de test :

```bash
cd backend
pip install -r requirements.txt
```

## ExÃ©cution des Tests

### Tous les tests (mocked uniquement)

```bash
pytest
```

### Tests unitaires seulement

```bash
pytest -m unit
```

### Tests d'intÃ©gration seulement

```bash
pytest -m integration
```

### Tests d'un fichier spÃ©cifique

```bash
pytest tests/unit/test_grist_service.py
```

### Tests avec verbositÃ©

```bash
pytest -v
pytest -vv  # Plus de dÃ©tails
```

### Tests avec coverage

```bash
pytest --cov=app --cov-report=html
# Ouvre htmlcov/index.html pour voir le rapport
```

## Types de Tests

### Tests Unitaires (`@pytest.mark.unit`)

Tests rapides avec des mocks, sans dÃ©pendances externes :

```python
@pytest.mark.unit
class TestGristService:
    async def test_get_tables(self, mock_grist_service):
        tables = await mock_grist_service.get_tables()
        assert len(tables) == 3
```

**Avantages :**
- âš¡ TrÃ¨s rapides
- ğŸ”’ Pas besoin de credentials API
- ğŸ§ª IsolÃ©s et reproductibles
- ğŸš€ Peuvent tourner en CI/CD

### Tests d'IntÃ©gration (`@pytest.mark.integration`)

Tests avec des composants rÃ©els mais toujours mockÃ©s :

```python
@pytest.mark.integration
@pytest.mark.asyncio
class TestAgentWorkflows:
    async def test_workflow_list_tables(self, agent_with_mocks):
        result = await agent_with_mocks.run("What tables are in this document?")
        assert result["success"] is True
```

### Tests avec API RÃ©elle (`@pytest.mark.requires_api`)

Tests nÃ©cessitant une vraie connexion Ã  Grist :

```python
@pytest.mark.requires_api
@pytest.mark.integration
class TestAllToolsRealAPI:
    async def test_real_get_tables(self, real_grist_service):
        result = await get_tables.ainvoke({})
        assert isinstance(result, list)
```

## Tests avec API RÃ©elle

### Configuration

Pour exÃ©cuter les tests avec une vraie API Grist, configurez les variables d'environnement :

```bash
# CrÃ©ez un fichier .env dans backend/
GRIST_API_KEY=votre_clÃ©_api_ici
GRIST_DOCUMENT_ID=votre_document_id_ici  # Optionnel
GRIST_BASE_URL=https://grist.numerique.gouv.fr  # Optionnel
```

### Obtenir une API Key Grist

1. Connectez-vous Ã  votre instance Grist
2. Allez dans **Account Settings** (ParamÃ¨tres du compte)
3. Section **API** â†’ CrÃ©ez une nouvelle clÃ©
4. Copiez la clÃ© dans votre `.env`

### ExÃ©cution

```bash
# ExÃ©cuter seulement les tests avec API rÃ©elle
pytest -m requires_api

# ExÃ©cuter un test spÃ©cifique avec API rÃ©elle
pytest tests/integration/test_all_tools.py::TestAllToolsRealAPI::test_real_get_tables -m requires_api

# Avec verbositÃ© pour voir les dÃ©tails
pytest -m requires_api -v
```

### âš ï¸ Avertissement

Les tests avec API rÃ©elle :
- CrÃ©ent des donnÃ©es temporaires dans votre document Grist
- Une table `TestToolsTable` sera crÃ©Ã©e
- **Cette table ne peut PAS Ãªtre supprimÃ©e automatiquement** (limitation API Grist)
- Vous devez la supprimer manuellement via l'interface Grist

## Configuration

### Fixtures Disponibles

Toutes dÃ©finies dans `conftest.py` :

#### Configuration
- `test_settings` - Configuration de test avec valeurs par dÃ©faut

#### DonnÃ©es de Test
- `sample_tables` - Exemples de tables Grist
- `sample_columns` - Exemples de colonnes
- `sample_records` - Exemples d'enregistrements

#### Mocks
- `mock_grist_client` - Client API mockÃ©
- `mock_grist_service` - Service Grist mockÃ©
- `mock_llm` - LLM LangChain mockÃ©

#### API Tests
- `api_client` - Client FastAPI pour tests
- `sample_chat_request` - RequÃªte chat exemple
- `sample_headers` - Headers HTTP exemple

### Markers Pytest

DÃ©finis dans `pytest.ini` :

- `@pytest.mark.unit` - Tests unitaires
- `@pytest.mark.integration` - Tests d'intÃ©gration
- `@pytest.mark.slow` - Tests lents
- `@pytest.mark.requires_api` - Tests nÃ©cessitant API rÃ©elle

### Utilisation des Fixtures

```python
@pytest.mark.unit
@pytest.mark.asyncio
class TestMyFeature:
    async def test_something(self, mock_grist_service, sample_tables):
        """Test utilisant les fixtures."""
        tables = await mock_grist_service.get_tables()
        assert len(tables) == len(sample_tables)
```

## Bonnes Pratiques

### âœ… Ã€ Faire

1. **Utiliser les fixtures** du `conftest.py`
2. **Marquer les tests** avec les bons decorators
3. **Tester avec mocks** par dÃ©faut
4. **Nettoyer** aprÃ¨s les tests
5. **Documenter** les cas de test complexes

### âŒ Ã€ Ã‰viter

1. âŒ Hardcoder des credentials dans les tests
2. âŒ CrÃ©er des mocks custom quand une fixture existe
3. âŒ Laisser des donnÃ©es rÃ©siduelles
4. âŒ Tests avec side effects non documentÃ©s
5. âŒ Tests trop longs ou trop lents sans marker `@pytest.mark.slow`

## Debugging

### Voir les logs complets

```bash
pytest --log-cli-level=DEBUG
```

### ArrÃªter au premier Ã©chec

```bash
pytest -x
```

### ExÃ©cuter seulement les tests qui ont Ã©chouÃ©

```bash
pytest --lf  # Last failed
```

### Mode debug avec pdb

```bash
pytest --pdb
```

### Voir les prints dans les tests

```bash
pytest -s
```

## CI/CD

Les tests sont automatiquement exÃ©cutÃ©s dans la CI. Seuls les tests mocked (sans `@pytest.mark.requires_api`) tournent automatiquement.

Pour inclure les tests API rÃ©els dans la CI, configurez les secrets :
- `GRIST_API_KEY`
- `GRIST_DOCUMENT_ID`
- `GRIST_BASE_URL`

## Contribuer

Lors de l'ajout de nouveaux tests :

1. Suivez la structure existante
2. Utilisez les fixtures partagÃ©es
3. Ajoutez les markers appropriÃ©s
4. Documentez les cas complexes
5. Assurez-vous que les tests passent : `pytest`

## Exemples

### Test Unitaire Simple

```python
@pytest.mark.unit
@pytest.mark.asyncio
async def test_get_tables(mock_grist_service, sample_tables):
    """Test simple avec mock."""
    tables = await mock_grist_service.get_tables()
    assert len(tables) == 3
    assert tables[0]["id"] == "Students"
```

### Test d'IntÃ©gration

```python
@pytest.mark.integration
@pytest.mark.asyncio
class TestWorkflow:
    async def test_complete_workflow(self, agent_with_mocks):
        """Test workflow complet."""
        result = await agent_with_mocks.run("Add a student")
        assert result["success"] is True
```

### Test avec API RÃ©elle

```python
@pytest.mark.requires_api
@pytest.mark.integration
@pytest.mark.asyncio
async def test_real_api(real_grist_service):
    """Test avec vraie API."""
    tables = await real_grist_service.get_tables()
    assert isinstance(tables, list)
```

## Support

Pour toute question sur les tests :
- Consultez la documentation : `docs/`
- Ouvrez une issue sur GitHub
- Contactez l'Ã©quipe de dÃ©veloppement

